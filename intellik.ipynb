{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IFTHISDOESNTWORKILLKMS.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9Rxio7TvP9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DcjIXUhdzYx",
        "colab_type": "code",
        "outputId": "e47ffb7a-fae9-4d35-ca77-f576f148e59f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "!pip install tensorflow==1.7.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/83/35c3f53129dfc80d65ebbe07ef0575263c3c05cc37f8c713674dcedcea6f/tensorflow-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (48.1MB)\n",
            "\u001b[K     |████████████████████████████████| 48.1MB 66kB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7.1) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7.1) (1.27.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7.1) (0.34.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7.1) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7.1) (1.12.0)\n",
            "Collecting tensorboard<1.8.0,>=1.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/ec/65d4e8410038ca2a78c09034094403d231228d0ddcae7d470b223456e55d/tensorboard-1.7.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 49.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7.1) (0.9.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7.1) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7.1) (1.17.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7.1) (1.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.1) (3.2.1)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 50.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.1) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.7.1) (45.1.0)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107220 sha256=54f7778ccd220610821741f387aadfb2b63466df1fe0b16bf85b1505a9a242f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "\u001b[31mERROR: magenta 0.3.19 has requirement tensorflow>=1.12.0, but you'll have tensorflow 1.7.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: html5lib, bleach, tensorboard, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.0\n",
            "    Uninstalling bleach-3.1.0:\n",
            "      Successfully uninstalled bleach-3.1.0\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-1.7.0 tensorflow-1.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtEKIUF-LFb3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "060f4303-b25b-4732-d9ff-85f31f9303b3"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcMr_0Z1dtFN",
        "colab_type": "code",
        "outputId": "80ed0f16-ba6d-4a81-dade-2b8f888b1db8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.7.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXCMb8mqvezT",
        "colab_type": "code",
        "outputId": "3cdd5490-9adb-41c1-c31d-8514d79182eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3znLlI_vfCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "RAWDATADIR = '/content/drive/My Drive/IntelliK/dataset6'\n",
        "NEWDATADIR = '/content/drive/My Drive/IntelliK/dataset_4848'\n",
        "CATEGORIES = ['U','D','L','R', 'K', 'nothing']\n",
        "\n",
        "from tensorflow.python.framework import graph_util\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "from tensorflow.python.platform import gfile\n",
        "from tensorflow.python.util import compat\n",
        "\n",
        "# def create_training_data():\n",
        "#   for category in CATEGORIES:\n",
        "#     path = os.path.join(NEWDATADIR, category)\n",
        "#     class_num = CATEGORIES.index(category)\n",
        "#     for img in os.listdir(path):\n",
        "\n",
        "#       img_ar = cv2.imread(os.path.join(path, img))\n",
        "#       img_ar = np.array(img_ar)\n",
        "      \n",
        "#       training_data.append([img_ar,class_num])\n",
        "\n",
        "# training_data = []\n",
        "# create_training_data()\n",
        "# print(len(training_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofqop7kBvfHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# import random\n",
        "# random.shuffle(training_data)\n",
        "# X = []\n",
        "# y = []\n",
        "# for features, label in training_data:\n",
        "#   X.append(features)\n",
        "#   y.append(label)\n",
        "# X = np.array(X)\n",
        "# print(X.shape)\n",
        "\n",
        "# X = np.array(X).reshape(-1,48,48,3)\n",
        "# print(y[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwkVSr6pvfLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import pickle\n",
        "# pickle_out = open('/content/drive/My Drive/IntelliK/X1.pickle', 'wb')\n",
        "# pickle.dump(X, pickle_out)\n",
        "# pickle_out.close()\n",
        "# pickle_out = open('/content/drive/My Drive/IntelliK/y1.pickle', 'wb')\n",
        "# pickle.dump(y, pickle_out)\n",
        "# pickle_out.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGfgAqwQvhDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "pickle_in = open('/content/drive/My Drive/IntelliK/X1.pickle', 'rb')\n",
        "X = pickle.load(pickle_in)\n",
        "pickle_in = open('/content/drive/My Drive/IntelliK/y1.pickle', 'rb')\n",
        "y = pickle.load(pickle_in)\n",
        "y = np.array(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2cCXC8nvhQS",
        "colab_type": "code",
        "outputId": "6aea48e7-7251-425e-e30e-a90382107bc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print('X: train, test')\n",
        "print(len(X_train))\n",
        "\n",
        "print(len(X_test))\n",
        "print('y: train, test')\n",
        "print(len(y_train))\n",
        "\n",
        "print(len(y_test))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X: train, test\n",
            "4050\n",
            "450\n",
            "y: train, test\n",
            "4050\n",
            "450\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iSMX4gjvhS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KYa9nEmxwep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq_FXrAQvfTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weight_variable(shape):\n",
        "  initial = tf.truncated_normal(shape,stddev=0.1)\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "  initial = tf.constant(0.1, shape = shape)\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x, W):\n",
        "  return tf.nn.conv2d(x,W,strides=[1,1,1,1], padding = 'SAME')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "  return tf.nn.max_pool(x, ksize= [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
        "\n",
        "def dropout(x,prob,nameoftensor):\n",
        "  return tf.nn.dropout(x,prob,name = nameoftensor)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWiWcYdqxvy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJkXrrUNvfWm",
        "colab_type": "code",
        "outputId": "13080518-1abb-4a11-9e96-cfd25bde2c3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "graph1 = tf.Graph()\n",
        "with graph1.as_default(): #MODEL ARCHITECHTURE, CURRENTLY 3 PADDED CONV LAYERS AND 1 FC LAYER\n",
        "  input_data = tf.placeholder(tf.float32, [None, 48,48,3],name = '48x48')\n",
        "  output_data = tf.placeholder(tf.float32, [None,6],name = 'groundtruth')\n",
        "  keep_prob = tf.placeholder(tf.float32, shape = (), name = 'dropout')\n",
        "\n",
        "  #first 3x3 conv, taking in 48x48x3 and output 24x24x32\n",
        "  Weight_conv1 = weight_variable([3,3,3,32])\n",
        "  bias_conv1 = bias_variable([32])\n",
        " \n",
        "  relu_conv1 = tf.nn.relu(conv2d(input_data, Weight_conv1) + bias_conv1)\n",
        "  h_pool1 = max_pool_2x2(dropout(relu_conv1,keep_prob,'1'))\n",
        "\n",
        "  #second 5x5 conv, taking in 24x24x32 and output 12x12x64\n",
        "  Weight_conv2 = weight_variable([5,5,32,64])\n",
        "  bias_conv2 = bias_variable([64])\n",
        "\n",
        "  relu_conv2 = tf.nn.relu(conv2d(h_pool1, Weight_conv2) + bias_conv2)\n",
        "  h_pool2 = max_pool_2x2(dropout(relu_conv2,keep_prob,'2'))\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "  #third 5x5 conv, taking in 12x12x64 and output 6x6x128\n",
        "  Weight_conv3 = weight_variable([5,5,64,128])\n",
        "  bias_conv3= bias_variable([128])\n",
        "\n",
        "  relu_conv3 = tf.nn.relu(conv2d(h_pool2, Weight_conv3) + bias_conv3)\n",
        "  h_pool3 = max_pool_2x2(dropout(relu_conv3,keep_prob,'3'))\n",
        "\n",
        "  #fc layer, taking in 12X12X64, flattening it and output 6 (one for each class)\n",
        "  Weight_fc1 = weight_variable([6*6*128,6])\n",
        "  bias_fc1 = bias_variable([6])\n",
        "\n",
        "  h_pool3_flat = tf.reshape(h_pool3,[-1,6*6*128])\n",
        "  h_fc1 = tf.matmul(h_pool3_flat, Weight_fc1, name = 'model_end')\n",
        "  softmaxed = tf.nn.softmax(h_fc1, name = 'softmax')\n",
        "\n",
        "\n",
        "beta = 0.1\n",
        "with tf.Session(graph=graph1) as sess:\n",
        "  cross_entropy = tf.reduce_mean(\n",
        "      tf.nn.softmax_cross_entropy_with_logits_v2(labels = output_data, logits = h_fc1)\n",
        "  )\n",
        "  \n",
        "  regularizers = tf.nn.l2_loss(Weight_conv1) +  tf.nn.l2_loss(Weight_conv2) + tf.nn.l2_loss(Weight_conv3)\n",
        "  \n",
        "  train_step = tf.train.AdamOptimizer(3e-5).minimize(cross_entropy + beta * regularizers) #LEARNING RATE\n",
        "  correct_prediction = tf.equal(tf.argmax(h_fc1,1),tf.argmax(output_data,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  \n",
        "  batch = [X_train,y_train]\n",
        "  batch_size = 50\n",
        "  total_batch = len(X_train)//batch_size\n",
        "  \n",
        "  for i in range(150): #NUMBER OF EPOCHS\n",
        "    if i%10 == 0:\n",
        "      train_accuracy = accuracy.eval(feed_dict={\n",
        "          input_data:batch[0], output_data:batch[1], keep_prob:1\n",
        "      })\n",
        "      print('step %d, training accuracy %g'%(i, train_accuracy))\n",
        "    for j in range(total_batch-1):\n",
        "      batch_x = X_train[batch_size * j:batch_size * (j+1)]\n",
        "      batch_y = y_train[batch_size * j:batch_size * (j+1)]\n",
        "      sess.run(train_step,feed_dict={\n",
        "          input_data:batch_x, output_data:batch_y, keep_prob:0.6\n",
        "      })\n",
        "      \n",
        "\n",
        "  print('test accuracy %g'%accuracy.eval(feed_dict={\n",
        "          input_data:X_test, output_data:y_test, keep_prob:1\n",
        "      }))\n",
        "  output_graph_def = graph_util.convert_variables_to_constants(\n",
        "                sess, tf.get_default_graph().as_graph_def(), ['softmax'])\n",
        "  with gfile.FastGFile('/content/drive/My Drive/IntelliK/output_150_withsm.pb', 'wb') as f: #NAME OF MODEL TO BE SAVED\n",
        "    f.write(output_graph_def.SerializeToString())\n",
        "  with gfile.FastGFile('/content/drive/My Drive/IntelliK/outputlabels.txt', 'w') as f: #NAME OF LABELS, NO NEED TO CHANGE\n",
        "    f.write('\\n'.join(CATEGORIES) + '\\n')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, training accuracy 0.170864\n",
            "step 10, training accuracy 0.460247\n",
            "step 20, training accuracy 0.561235\n",
            "step 30, training accuracy 0.662469\n",
            "step 40, training accuracy 0.705432\n",
            "step 50, training accuracy 0.755309\n",
            "step 60, training accuracy 0.806914\n",
            "step 70, training accuracy 0.839012\n",
            "step 80, training accuracy 0.883457\n",
            "step 90, training accuracy 0.905432\n",
            "step 100, training accuracy 0.928148\n",
            "step 110, training accuracy 0.929877\n",
            "step 120, training accuracy 0.948642\n",
            "step 130, training accuracy 0.955309\n",
            "step 140, training accuracy 0.962469\n",
            "test accuracy 0.966667\n",
            "INFO:tensorflow:Froze 7 variables.\n",
            "Converted 7 variables to const ops.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPNGy9UK24Zg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UFa8FA-3Kya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVi6ReuH3Peq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
